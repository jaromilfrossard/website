---
title: 'Playing with gganimate: Robust Estimation'
author: Jaromil Frossard
date: '2019-11-01'
slug: []
categories:
  - R
tags:
  - robust statistic
  - R
  - gganimate
subtitle: ''
summary: ''
authors: []
lastmod: '2019-11-01T10:03:17+01:00'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: yes
projects: []
url_code: "script/2019_11_01_robust.R"
bibliography: [post.bib]
---

# Robust Estimation

OLS is not robust to outliers. It is computed by minimizing the sum of squares of the residuals and each outlying observation has a large residual and consequently a large effect on this sum of squares. On the other hand, The M-estimators used in robust statistics [@heritier_robust_2009;@huber_robust_1964;@maronna_robust_2006] are not influenced by outling data. @huber_robust_1964 proposed to minimize functions which are less influenced by outliers rather than the sum of squares. This functions are a central to the theory of robust statistics and are called $\rho$-function. Their derivative $\psi(r) = \frac{\partial}{\partial r}\rho(r)$ are also useful as miniminzing $\rho(r)$ is equivalent to solving $\psi(r)=0$. Moreover, we can infer robust properties of an estimator from its $\psi$-function. $\psi$-functions can be un-bounded (like OLS), bounded (like Huber M-estimator) or bounded and redescending (like the bi-square redescending in @koller_sharpening_2011).


```{r psi, echo=FALSE, message=FALSE, warning=FALSE}
library(robustbase)
library(tidyr)
library(dplyr)
library(ggplot2)
x = seq(from = -5,to=5,length.out = 80)
df_ols = data.frame(residual = x,
                    psi = x)
df_ols$method = "OLS"

psi <- "huber"
df_h = data.frame(residual = x,
                  psi = Mpsi(x,psi=psi,cc= .Mpsi.tuning.default(psi)))
df_h$method = "Huber"

psi <- "bisquare"
df_bi = data.frame(residual = x,
                   psi = Mpsi(x,psi=psi,cc= .Mpsi.tuning.default(psi)))
df_bi$method = "Bisquare redescending"

df = rbind(df_ols,df_h,df_bi)

df%>%
  mutate(method = factor(method, levels =  c("OLS", "Huber","Bisquare redescending")))%>%
  ggplot(aes(x=residual,y=psi,group=method,colour=method))+
  geom_hline(yintercept = 0,col="grey")+
  geom_line(lwd=1.5)+
  scale_colour_manual(name="", values = c(`Huber`="red",OLS="black",`Bisquare redescending` = "orange"),guide=FALSE)+
  theme_bw()+
  xlab(label = "r")+
  ylab((expression(psi)))+
  theme(title = element_text(size=13),
        strip.text.x = element_text(size=13),
        strip.background = element_rect(colour = "black",fill = NA),
        panel.border = element_rect(colour = "black",fill = NA),
        panel.grid = element_blank(),
        axis.text.y = element_text(size = 13),
        axis.text.x = element_text(size = 13))+
  facet_wrap(~method)+ coord_fixed()


```



The OLS has a $\psi$-function increasing wich results in large effect of the outliers on the estimation. The Huber $\psi$-function is bounded and outliers have finite effects even if the outlying point "goes" to infinity. Finally, the bisquare resdescending $\psi$-function consider the influence of large outliers as null.

Using R in regression, the rlm() function from the MASS package [@venables_modern_2013] computes the Huber estimator and the lmrob() function from the robustbase [@maechler_robustbase_2016] package uses the bi-square redescending $\psi$-function. Here are an animation [@pedersen_gganimate_2019] showing this effect on the estimation on the intercept and slopes:


![](/img/robust.gif)

# References


