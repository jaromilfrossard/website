---
title: 'Playing with gganimate: Robust Estimation'
author: Jaromil Frossard
date: '2019-11-01'
slug: []
categories:
  - R
tags:
  - robust statistic
  - R
  - gganimate
subtitle: ''
summary: ''
authors: []
lastmod: '2019-11-01T10:03:17+01:00'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: yes
projects: []
url_code: "script/2019_11_01_robust.R"
bibliography: [post.bib]
---

# Robust Estimation

The M-estimators used in robust statistics [@heritier_robust_2009;@huber_robust_1964;@maronna_robust_2006] are not influenced by outling data. On the other hand, OLS is not robust to outliers as it is computed by minimizing the sum of squares of the residuals as each outlying observation will have a large residual and also a large effect on this sum of squares. @huber_robust_1964 proposed to minimize other functions which are less influenced by outliers. This function are a central theory of robust statistics and are called $\rho$-function. The derivative $\psi(r) = \frac{\partial}{\partial r}\rho(r)$ is also useful as miniminzing is equivalent to solving $\psi(r)$. Moreover, we can infer robust properties of an estimator from its $\psi$-function. $\psi$-functions can be un-bounded (like OLS), bounded (like Huber M-estimator) or bounded and redescending (like the bi-square redescending in @koller_sharpening_2011).


```{r psi, echo=FALSE, message=FALSE, warning=FALSE}
library(robustbase)
library(tidyr)
library(dplyr)
library(ggplot2)
x = seq(from = -5,to=5,length.out = 80)
df_ols = data.frame(residual = x,
                    psi = x)
df_ols$method = "OLS"

psi <- "huber"
df_h = data.frame(residual = x,
                  psi = Mpsi(x,psi=psi,cc= .Mpsi.tuning.default(psi)))
df_h$method = "Huber"

psi <- "bisquare"
df_bi = data.frame(residual = x,
                   psi = Mpsi(x,psi=psi,cc= .Mpsi.tuning.default(psi)))
df_bi$method = "Bisquare redescending"

df = rbind(df_ols,df_h,df_bi)

df%>%
  mutate(method = factor(method, levels =  c("OLS", "Huber","Bisquare redescending")))%>%
  ggplot(aes(x=residual,y=psi,group=method,colour=method))+
  geom_hline(yintercept = 0,col="grey")+
  geom_line(lwd=1.5)+
  scale_colour_manual(name="", values = c(`Huber`="red",OLS="black",`Bisquare redescending` = "orange"),guide=FALSE)+
  theme_bw()+
  xlab(label = "r")+
  ylab((expression(psi)))+
  theme(title = element_text(size=13),
        strip.text.x = element_text(size=13),
        strip.background = element_rect(colour = "black",fill = NA),
        panel.border = element_rect(colour = "black",fill = NA),
        panel.grid = element_blank(),
        axis.text.y = element_text(size = 13),
        axis.text.x = element_text(size = 13))+
  facet_wrap(~method)+ coord_fixed()


```



The OLS has a $\psi$-function increasing wich results in large effect of the outliers on the estimation. The Huber $\psi$-function is bounded and outliers have finite effects even if the outlying point "goes" to infinity. Finally, the bisquare resdescending $\psi$-function consider the influence of large outliers as null.

Using R in regression, the rlm() function from the MASS package [@venables_modern_2013] computes the Huber estimator and the lmrob() function from the robustbase [@maechler_robustbase_2016] package uses the bi-square redescending $\psi$-function. Here are an animation [@pedersen_gganimate_2019] showing this effect on the estimation on the intercept and slopes:


![](/img/robust.gif)

# References


