---
title: ANOVA and LMM
author: Jaromil Frossard
date: '2017-10-26'
slug: anova-and-lmm
categories:
  - R
tags: []
summary: Using lme4 to estimate ANOVA models
#bibliography: post.bib
---



<div id="some-notations" class="section level1">
<h1>Some notations</h1>
<p>forthcoming</p>
<!-- The underlying model of the repeated measures ANOVA can be written as a mixed linear models. However mixed models packages like lme4 [@bates_fitting_2015] makes the estimation of such models not intuitive.  -->
<!-- For a repeated measures ANOVA with 2 within factors, we write the following model using the notation in @howell_statistical_2012 :  -->
<!-- $$ -->
<!-- y_{ijk} = \mu + \alpha_j + \psi_k + (\alpha\psi)_{jk}+\pi_i +(\pi\alpha)_{ij}+(\pi\psi)_{ik} +(\pi\alpha\psi)_{ijk}+\epsilon_{ijk}. -->
<!-- $$ -->
<!-- with  -->
<!-- - $y_{ijk}$ the response variable of the subject $i \in \{1,\dots, n\}$ evaluated at the level $j \in \{1,\dots, a\}$ of the factor $\psi$ and at the level $k \in \{1,\dots, b\}$ of the factor $\alpha$. -->
<!-- - $\alpha_j$, $\psi_k$,  are the fixed effects of the two factors. -->
<!-- - $(\alpha\psi)_{jk}$ are the fixed effect of the interaction. -->
<!-- - $\pi_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi})$, $(\pi\psi)_{ik} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi\psi})$, $(\pi\alpha)_{ij}\overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi\alpha})$ and $(\pi\alpha\psi)_{ijk}\overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi\alpha\psi})$ are random effects. They correspond to the random intercept of each subject, the random changes of the effect of, respectively, $\alpha$ and $\psi$ on individuals and the random change of their interactions. -->
<!-- - $\epsilon_{ijk} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\epsilon})$ are the random error. -->
<!-- - We assume a balanced design between the within factors. -->
<!-- We note that : -->
<!-- 1. All random effects are independent. -->
<!-- 2. The errors $\epsilon_{ijk}$ and the last interaction terms $(\pi\alpha\psi)_{ijk}$ are  confounded. -->
<!-- The same model is written in a mixed linear form as : -->
<!-- $$ -->
<!-- \mathbf{y} = \mathbf{X\beta} + \mathbf{Z\gamma} + \mathbf{\epsilon} -->
<!-- $$ -->
<!-- where $\mathbf{y}$ is the vector of response, $\mathbf{X}$ and $\mathbf{Z}$ are respectively the fixed and random design. $\mathbf{\beta}$ is the vector of fixed parameters, $\mathbf{\gamma}$ are the random parameters and $\mathbf{\epsilon}$ is the error term. @christensen_plane_2010 gives a more detailed explanation on the link between repeated measure ANOVA and LMM.  -->
<!-- # R code -->
<!-- We simulate data from an ANOVA following the formula above : -->
<!-- ```{r} -->
<!-- set.seed(42) -->
<!-- ## Parameters -->
<!-- n <- 10; a <- 3; b = 4 -->
<!-- sigmas <- c(id = 1, idpsi = 1, idalpha =1, idpsialpha = 1, epsilon = 1)  -->
<!-- beta <- c(1, round(runif(a-1)*10),round(runif(b-1)*10),round(runif((a-1)*(b-1))*10)) -->
<!-- ## Data frame -->
<!-- data <- expand.grid(id = paste0("s",1:a),  alpha = paste0("alpha",1:a),psi = paste0("psi",1:b)) -->
<!-- ## Fix design -->
<!-- contrasts(data$alpha) <- contr.sum -->
<!-- contrasts(data$psi) <- contr.sum -->
<!-- contrasts(data$id) <- contr.sum -->
<!-- x <- model.matrix(~alpha*psi, data) -->
<!-- ## Random design -->
<!-- z_id <- model.matrix(~id-1, data) -->
<!-- z_idalpha <- model.matrix(~id:alpha-1, data) -->
<!-- z_idpsi <- model.matrix(~id:psi-1, data) -->
<!-- z_idalphapsi <- model.matrix(~id:alpha:psi-1, data) -->
<!-- z_epsilon <- diag(nrow(data)) -->
<!-- # List of the random designs -->
<!-- zlist = list(z_id = z_id, z_idpsi = z_idpsi, z_idalpha = z_idalpha, z_idalphapsi = z_idalphapsi, z_epsilon = z_epsilon) -->
<!-- ## Covariance matrix -->
<!-- omega <- mapply(function(zi,si){zi%*%t(zi)*si^2}, zi = zlist, si = sigmas, SIMPLIFY = F) -->
<!-- omega <- Reduce(f = "+", omega) -->
<!-- ## Simulate the response -->
<!-- library(MASS) -->
<!-- data$y = mvrnorm(n =1  , mu=as.numeric(x%*%beta),Sigma = omega) -->
<!-- ``` -->
<!-- ## Repeated measures ANOVA with aov() -->
<!-- The aov() function performs ANOVA with type 1 sums of square. However in our fully balanced design, it will provide the same results than the type 3. The output gives F test and exact p-values under the assumptions. The Greenhouse-Geisser or Huynh-Feldt correction are often used to compute p-values bevause we cannot trust the assumption ares fully fulfilled. In that case or if you have a unbalanced design (and you want to use type 3 sum of squares), I recommend to use the afex package [@singmann_afex:_2017]. -->
<!-- The formula of aov() should be written using $+ Error(subject/(within\_factors))$ to specify the within factors in the model.  -->
<!-- ```{r} -->
<!-- summary(aov(y ~ alpha*psi + Error(id/(alpha*psi)), data)) -->
<!-- ``` -->
<!-- ## Miss-use of lme4 formula with factors  -->
<!-- The formula of lme4 allows user to specify several type of random design. However it handles the factor in an un-intuitive way and if the random structure is miss-specified the estimation will fail to converge. -->
<!-- If we understand the two random effects $(\pi\alpha)_{ij}$ and $(\pi\psi)_{ik}$ as random slopes and we specify the random strucutre using the notation $+ (within\_factor|subject)$. The ANOVA model does not assume covariance between random slope and random intercept and we can implement this feature using the double bar symbol $( \cdot || \cdot )$ in the formula. Moreover the higher level interaction is confounded with the error and is not estimable. This let us with the following formula : -->
<!-- ```{r, echo=T, eval=F} -->
<!-- y ~ alpha * psi + (alpha + psi || id) -->
<!-- ``` -->
<!-- However this formula does not match the assumed model and estimate more parameters than the assumed model does. We will first estimated different variances for each random slopes in the same "group" ($Var \left[  (\pi\alpha)_{ij}\right] = Var\left[  (\pi\alpha)_{i'j'}\right]$) and secondly we estimate covariance between the random "slopes". Using this formula the assumed covariance structure is composed of $1 + (a - 1) + (b -1)$ variance parameters (  1 + a + b in our case) and $a(a-1)/2 + b(b-1)/2$ covariance parameters (  a*(a-1)/2 + b*(b-1)/2 in our case). However the ANOVA model has only 3 parameters of variance (+ 1 confounded with the error term) and 0 covariance parameter. -->
<!-- Running the lmer function with this formula will not converge :  -->
<!-- ```{r warning=FALSE} -->
<!-- library(lme4) -->
<!-- m1 <- lmer(y ~ alpha*psi + (alpha + psi||id), data) -->
<!-- ## failure to converge -->
<!-- m1@optinfo$conv$lme4$messages -->
<!-- ## Size of the parameter vector -->
<!-- length(getME(m1,"theta")) -->
<!-- ``` -->
<!-- A solution available in the lme4 package [@bates_fitting_2015] is to benefits from the modular construction of the lmer formula. Its allows a large flexibility on the random structure you want to estimate but it need a good understanding of the estimation of the model (and some programming skills). -->
<!-- ```{r} -->
<!-- ## Create the model to estmated -->
<!-- parsedFormula <- lFormula(y ~ alpha*psi + (alpha + psi||id), data) -->
<!-- ## Modify the random structure -->
<!-- parsedFormula$reTrms <- within(parsedFormula$reTrms, { -->
<!--   # number of variance -->
<!--   nvar <- sapply(Ztlist,nrow)  -->
<!--   # initilize paramters -->
<!--   theta <- rep(1,length(nvar))  -->
<!--   # initilize lower bound -->
<!--   lower <- rep(0,length(nvar)) -->
<!--   # assign the same variances for each slopes -->
<!--   Lind <-  do.call("c",sapply(1:length(nvar),function(i)rep(i,nvar[i]))) -->
<!--   # recompute the Lambda matrix -->
<!--   q <- nrow(Lambdat) -->
<!--   Lambdat <- sparseMatrix(1:q, 1:q, x = theta[Lind]) -->
<!-- }) -->
<!-- ## Create the optimal function -->
<!-- devianceFunction <- do.call(mkLmerDevfun, parsedFormula) -->
<!-- ## Find optimum -->
<!-- optimizerOutput <- optimizeLmer(devianceFunction) -->
<!-- ## check convergence -->
<!-- optimizerOutput$convergence -->
<!-- ## number of parameter -->
<!-- length(optimizerOutput$par) -->
<!-- ``` -->
<!-- The model converge and the parameter vector is of the right dimension. However the mkMerMod function does not handle the manual change in the optimal function and we need to compute manually all other statistics from the optimal parameter, and the design. This cannot be viable solution for all users. -->
<!-- ## Handeling factors with lme4 -->
<!-- The approach explained in [@bates_fitting_2015] and in details in the following [mailing list](https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001736.html) gives us a more feasible solution. Instead of modeling the two random effects $(\pi\alpha)_{ij}$ and $(\pi\psi)_{ik}$ as random changes of slopes and write the formula $+ (factor|id)$, we think of them as random change of intercept at the interaction level $+ (1|factor:id)$. We can then write the lmer formula :   -->
<!-- ```{r, echo=T, eval=T} -->
<!-- m2 <- lmer(y ~ alpha * psi + ( 1 | id)+ ( 1 | id:alpha) + ( 1 | id:psi), data) -->
<!-- ## Same optimal -->
<!-- c(optimizerOutput$fval, getME(m2,"devfun")(getME(m2,"theta"))) -->
<!-- ## Same parameter (at a permutation) -->
<!-- rbind(optimizerOutput$par[3:1], getME(m2,"theta")) -->
<!-- ## Full lmerMod object. -->
<!-- m2 -->
<!-- ``` -->
<!-- Based on this formula we find a way to estimate the same model than the ANOVA assume using lmer. The two notations assumed different random structures and can be resumed for the random effect $\gamma_A$: -->
<!-- 1. $+ (A|id)$ assumes $\gamma_A\sim \mathcal{N}(0,\Omega_A))$ -->
<!-- 2. $+ (1|A:id)$ assumes $\gamma_A\sim \mathcal{N}(0,I\sigma_A^2)$ -->
<!-- The first solution has the advantage to reduce the number of assumption on the covariance structure. The cost of this freedom is all the parameters in the $\Omega_A$ matrix, that increase with the number of levels of $A$. The second solution puts more assumption but reduce the number of parameters and gives more chance to the algorithm to converge. This solution may also be used with multiple random effects, like the subject and the item and their interaction. However finding an algorithm that converge gives us no information about the true model. If we estimate a model without covariances but the data are generated with covariance, the inference should be wrong. -->
<!-- # Bibliography -->
</div>
