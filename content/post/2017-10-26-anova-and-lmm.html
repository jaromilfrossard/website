---
title: ANOVA and LMM
author: Jaromil Frossard
date: '2017-10-26'
slug: anova-and-lmm
categories:
  - R
tags: []
summary: Using lme4 to estimate ANOVA models
bibliography: post.bib
---



<div id="some-notations" class="section level1">
<h1>Some notations</h1>
<p>The underlying model of the repeated measures ANOVA can be written as a mixed linear models. However mixed models packages like lme4 <span class="citation">(<span class="citeproc-not-found" data-reference-id="bates_fitting_2015"><strong>???</strong></span>)</span> makes the estimation of such models not intuitive.</p>
<p>For a repeated measures ANOVA with 2 within factors, we write the following model using the notation in <span class="citation">Howell (2012)</span> :</p>
<p><span class="math display">\[
y_{ijk} = \mu + \alpha_j + \psi_k + (\alpha\psi)_{jk}+\pi_i +(\pi\alpha)_{ij}+(\pi\psi)_{ik} +(\pi\alpha\psi)_{ijk}+\epsilon_{ijk}.
\]</span></p>
<p>with</p>
<ul>
<li><span class="math inline">\(y_{ijk}\)</span> the response variable of the subject <span class="math inline">\(i \in \{1,\dots, n\}\)</span> evaluated at the level <span class="math inline">\(j \in \{1,\dots, a\}\)</span> of the factor <span class="math inline">\(\psi\)</span> and at the level <span class="math inline">\(k \in \{1,\dots, b\}\)</span> of the factor <span class="math inline">\(\alpha\)</span>.</li>
<li><span class="math inline">\(\alpha_j\)</span>, <span class="math inline">\(\psi_k\)</span>, are the fixed effects of the two factors.</li>
<li><span class="math inline">\((\alpha\psi)_{jk}\)</span> are the fixed effect of the interaction.</li>
<li><span class="math inline">\(\pi_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi})\)</span>, <span class="math inline">\((\pi\psi)_{ik} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi\psi})\)</span>, <span class="math inline">\((\pi\alpha)_{ij}\overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi\alpha})\)</span> and <span class="math inline">\((\pi\alpha\psi)_{ijk}\overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\pi\alpha\psi})\)</span> are random effects. They correspond to the random intercept of each subject, the random changes of the effect of, respectively, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\psi\)</span> on individuals and the random change of their interactions.</li>
<li><span class="math inline">\(\epsilon_{ijk} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2_{\epsilon})\)</span> are the random error.</li>
<li>We assume a balanced design between the within factors.</li>
</ul>
<p>We note that :</p>
<ol style="list-style-type: decimal">
<li>All random effects are independent.</li>
<li>The errors <span class="math inline">\(\epsilon_{ijk}\)</span> and the last interaction terms <span class="math inline">\((\pi\alpha\psi)_{ijk}\)</span> are confounded.</li>
</ol>
<p>The same model is written in a mixed linear form as :</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X\beta} + \mathbf{Z\gamma} + \mathbf{\epsilon}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{y}\)</span> is the vector of response, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span> are respectively the fixed and random design. <span class="math inline">\(\mathbf{\beta}\)</span> is the vector of fixed parameters, <span class="math inline">\(\mathbf{\gamma}\)</span> are the random parameters and <span class="math inline">\(\mathbf{\epsilon}\)</span> is the error term. <span class="citation">Christensen (2010)</span> gives a more detailed explanation on the link between repeated measure ANOVA and LMM.</p>
</div>
<div id="r-code" class="section level1">
<h1>R code</h1>
<p>We simulate data from an ANOVA following the formula above :</p>
<pre class="r"><code>set.seed(42)


## Parameters
n &lt;- 10; a &lt;- 3; b = 4
sigmas &lt;- c(id = 1, idpsi = 1, idalpha =1, idpsialpha = 1, epsilon = 1) 
beta &lt;- c(1, round(runif(a-1)*10),round(runif(b-1)*10),round(runif((a-1)*(b-1))*10))

## Data frame
data &lt;- expand.grid(id = paste0(&quot;s&quot;,1:a),  alpha = paste0(&quot;alpha&quot;,1:a),psi = paste0(&quot;psi&quot;,1:b))

## Fix design
contrasts(data$alpha) &lt;- contr.sum
contrasts(data$psi) &lt;- contr.sum
contrasts(data$id) &lt;- contr.sum
x &lt;- model.matrix(~alpha*psi, data)

## Random design
z_id &lt;- model.matrix(~id-1, data)
z_idalpha &lt;- model.matrix(~id:alpha-1, data)
z_idpsi &lt;- model.matrix(~id:psi-1, data)
z_idalphapsi &lt;- model.matrix(~id:alpha:psi-1, data)
z_epsilon &lt;- diag(nrow(data))

# List of the random designs
zlist = list(z_id = z_id, z_idpsi = z_idpsi, z_idalpha = z_idalpha, z_idalphapsi = z_idalphapsi, z_epsilon = z_epsilon)

## Covariance matrix
omega &lt;- mapply(function(zi,si){zi%*%t(zi)*si^2}, zi = zlist, si = sigmas, SIMPLIFY = F)
omega &lt;- Reduce(f = &quot;+&quot;, omega)

## Simulate the response
library(MASS)
data$y = mvrnorm(n =1  , mu=as.numeric(x%*%beta),Sigma = omega)</code></pre>
<div id="repeated-measures-anova-with-aov" class="section level2">
<h2>Repeated measures ANOVA with aov()</h2>
<p>The aov() function performs ANOVA with type 1 sums of square. However in our fully balanced design, it will provide the same results than the type 3. The output gives F test and exact p-values under the assumptions. The Greenhouse-Geisser or Huynh-Feldt correction are often used to compute p-values bevause we cannot trust the assumption ares fully fulfilled. In that case or if you have a unbalanced design (and you want to use type 3 sum of squares), I recommend to use the afex package <span class="citation">(<span class="citeproc-not-found" data-reference-id="singmann_afex:_2017"><strong>???</strong></span>)</span>.</p>
<p>The formula of aov() should be written using <span class="math inline">\(+ Error(subject/(within\_factors))\)</span> to specify the within factors in the model.</p>
<pre class="r"><code>summary(aov(y ~ alpha*psi + Error(id/(alpha*psi)), data))</code></pre>
<pre><code>## 
## Error: id
##           Df Sum Sq Mean Sq F value Pr(&gt;F)
## Residuals  2  50.92   25.46               
## 
## Error: id:alpha
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## alpha      2   5874  2937.1   296.8 4.48e-05 ***
## Residuals  4     40     9.9                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Error: id:psi
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## psi        3 3158.2  1052.7   361.2 3.64e-07 ***
## Residuals  6   17.5     2.9                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Error: id:alpha:psi
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## alpha:psi  6   6766  1127.6   717.7 1.28e-14 ***
## Residuals 12     19     1.6                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="miss-use-of-lme4-formula-with-factors" class="section level2">
<h2>Miss-use of lme4 formula with factors</h2>
<p>The formula of lme4 allows user to specify several type of random design. However it handles the factor in an un-intuitive way and if the random structure is miss-specified the estimation will fail to converge.</p>
<p>If we understand the two random effects <span class="math inline">\((\pi\alpha)_{ij}\)</span> and <span class="math inline">\((\pi\psi)_{ik}\)</span> as random slopes and we specify the random strucutre using the notation <span class="math inline">\(+ (within\_factor|subject)\)</span>. The ANOVA model does not assume covariance between random slope and random intercept and we can implement this feature using the double bar symbol <span class="math inline">\(( \cdot || \cdot )\)</span> in the formula. Moreover the higher level interaction is confounded with the error and is not estimable. This let us with the following formula :</p>
<pre class="r"><code>y ~ alpha * psi + (alpha + psi || id)</code></pre>
<p>However this formula does not match the assumed model and estimate more parameters than the assumed model does. We will first estimated different variances for each random slopes in the same “group” (<span class="math inline">\(Var \left[ (\pi\alpha)_{ij}\right] = Var\left[ (\pi\alpha)_{i&#39;j&#39;}\right]\)</span>) and secondly we estimate covariance between the random “slopes”. Using this formula the assumed covariance structure is composed of <span class="math inline">\(1 + (a - 1) + (b -1)\)</span> variance parameters ( 8 in our case) and <span class="math inline">\(a(a-1)/2 + b(b-1)/2\)</span> covariance parameters ( 9 in our case). However the ANOVA model has only 3 parameters of variance (+ 1 confounded with the error term) and 0 covariance parameter.</p>
<p>Running the lmer function with this formula will not converge :</p>
<pre class="r"><code>library(lme4)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre class="r"><code>m1 &lt;- lmer(y ~ alpha*psi + (alpha + psi||id), data)

## failure to converge
m1@optinfo$conv$lme4$messages</code></pre>
<pre><code>## [1] &quot;unable to evaluate scaled gradient&quot;                                       
## [2] &quot;Model failed to converge: degenerate  Hessian with 1 negative eigenvalues&quot;</code></pre>
<pre class="r"><code>## Size of the parameter vector
length(getME(m1,&quot;theta&quot;))</code></pre>
<pre><code>## [1] 17</code></pre>
<p>A solution available in the lme4 package <span class="citation">(<span class="citeproc-not-found" data-reference-id="bates_fitting_2015"><strong>???</strong></span>)</span> is to benefits from the modular construction of the lmer formula. Its allows a large flexibility on the random structure you want to estimate but it need a good understanding of the estimation of the model (and some programming skills).</p>
<pre class="r"><code>## Create the model to estmated
parsedFormula &lt;- lFormula(y ~ alpha*psi + (alpha + psi||id), data)

## Modify the random structure
parsedFormula$reTrms &lt;- within(parsedFormula$reTrms, {
  # number of variance
  nvar &lt;- sapply(Ztlist,nrow) 
  # initilize paramters
  theta &lt;- rep(1,length(nvar)) 
  # initilize lower bound
  lower &lt;- rep(0,length(nvar))
  # assign the same variances for each slopes
  Lind &lt;-  do.call(&quot;c&quot;,sapply(1:length(nvar),function(i)rep(i,nvar[i])))
  # recompute the Lambda matrix
  q &lt;- nrow(Lambdat)
  Lambdat &lt;- sparseMatrix(1:q, 1:q, x = theta[Lind])
})

## Create the optimal function
devianceFunction &lt;- do.call(mkLmerDevfun, parsedFormula)

## Find optimum
optimizerOutput &lt;- optimizeLmer(devianceFunction)

## check convergence
optimizerOutput$convergence</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>## number of parameter

length(optimizerOutput$par)</code></pre>
<pre><code>## [1] 3</code></pre>
<p>The model converge and the parameter vector is of the right dimension. However the mkMerMod function does not handle the manual change in the optimal function and we need to compute manually all other statistics from the optimal parameter, and the design. This cannot be viable solution for all users.</p>
</div>
<div id="handeling-factors-with-lme4" class="section level2">
<h2>Handeling factors with lme4</h2>
<p>The approach explained in <span class="citation">(<span class="citeproc-not-found" data-reference-id="bates_fitting_2015"><strong>???</strong></span>)</span> and in details in the following <a href="https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001736.html">mailing list</a> gives us a more feasible solution. Instead of modeling the two random effects <span class="math inline">\((\pi\alpha)_{ij}\)</span> and <span class="math inline">\((\pi\psi)_{ik}\)</span> as random changes of slopes and write the formula <span class="math inline">\(+ (factor|id)\)</span>, we think of them as random change of intercept at the interaction level <span class="math inline">\(+ (1|factor:id)\)</span>. We can then write the lmer formula :</p>
<pre class="r"><code>m2 &lt;- lmer(y ~ alpha * psi + ( 1 | id)+ ( 1 | id:alpha) + ( 1 | id:psi), data)

## Same optimal
c(optimizerOutput$fval, getME(m2,&quot;devfun&quot;)(getME(m2,&quot;theta&quot;)))</code></pre>
<pre><code>## [1] 125.88 125.88</code></pre>
<pre class="r"><code>## Same parameter (at a permutation)
rbind(optimizerOutput$par[3:1], getME(m2,&quot;theta&quot;))</code></pre>
<pre><code>##      id:psi.(Intercept) id:alpha.(Intercept) id.(Intercept)
## [1,]          0.5338494             1.150834      0.8684987
## [2,]          0.5338494             1.150834      0.8684986</code></pre>
<pre class="r"><code>## Full lmerMod object.
m2</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: y ~ alpha * psi + (1 | id) + (1 | id:alpha) + (1 | id:psi)
##    Data: data
## REML criterion at convergence: 125.88
## Random effects:
##  Groups   Name        Std.Dev.
##  id:psi   (Intercept) 0.6691  
##  id:alpha (Intercept) 1.4425  
##  id       (Intercept) 1.0886  
##  Residual             1.2534  
## Number of obs: 36, groups:  id:psi, 12; id:alpha, 9; id, 3
## Fixed Effects:
## (Intercept)       alpha1       alpha2         psi1         psi2  
##      0.3333       9.3220       8.7398       1.8589       8.5554  
##        psi3  alpha1:psi1  alpha2:psi1  alpha1:psi2  alpha2:psi2  
##      5.2817       5.7210       7.4915       0.2250       8.1566  
## alpha1:psi3  alpha2:psi3  
##      7.3600       4.0492</code></pre>
<p>Based on this formula we find a way to estimate the same model than the ANOVA assume using lmer. The two notations assumed different random structures and can be resumed for the random effect <span class="math inline">\(\gamma_A\)</span>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(+ (A|id)\)</span> assumes <span class="math inline">\(\gamma_A\sim \mathcal{N}(0,\Omega_A))\)</span></li>
<li><span class="math inline">\(+ (1|A:id)\)</span> assumes <span class="math inline">\(\gamma_A\sim \mathcal{N}(0,I\sigma_A^2)\)</span></li>
</ol>
<p>The first solution has the advantage to reduce the number of assumption on the covariance structure. The cost of this freedom is all the parameters in the <span class="math inline">\(\Omega_A\)</span> matrix, that increase with the number of levels of <span class="math inline">\(A\)</span>. The second solution puts more assumption but reduce the number of parameters and gives more chance to the algorithm to converge. This solution may also be used with multiple random effects, like the subject and the item and their interaction. However finding an algorithm that converge gives us no information about the true model. If we estimate a model without covariances but the data are generated with covariance, the inference should be wrong.</p>
</div>
</div>
<div id="bibliography" class="section level1 unnumbered">
<h1>Bibliography</h1>
<div id="refs" class="references">
<div id="ref-christensen_plane_2010">
<p>Christensen, Ronald. 2010. <em>Plane Ansmers to Complex Questions : The Theory of Linear Models</em>.</p>
</div>
<div id="ref-howell_statistical_2012">
<p>Howell, David C. 2012. <em>Statistical Methods for Psychology</em>. 8 edition. Belmont, CA: Wadsworth Publishing.</p>
</div>
</div>
</div>
